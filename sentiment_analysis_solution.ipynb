{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094fa655",
   "metadata": {},
   "source": [
    "# Word embedding and RNN for sentiment analysis\n",
    "\n",
    "The goal of the following notebook is to predict whether a written\n",
    "critic about a movie is positive or negative. For that we will try\n",
    "three models. A simple linear model on the word embeddings, a\n",
    "recurrent neural network and a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd09a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "import appdirs                  # Used to cache pretrained embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext import datasets\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98d925",
   "metadata": {},
   "source": [
    "## The IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_cache = appdirs.user_cache_dir(\"pytorch\")\n",
    "train_iter, test_iter = datasets.IMDB(root=torch_cache, split=(\"train\", \"test\"))\n",
    "\n",
    "import random\n",
    "\n",
    "TRAIN_SET = list(train_iter)\n",
    "TEST_SET = list(test_iter)\n",
    "random.shuffle(TRAIN_SET)\n",
    "random.shuffle(TEST_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bd9be",
   "metadata": {},
   "source": [
    "## Global variables\n",
    "\n",
    "First let's define a few variables. `EMBEDDING_DIM` is the dimension\n",
    "of the vector space used to embed all the words of the vocabulary.\n",
    "`SEQ_LENGTH` is the maximum length of a sequence, `BATCH_SIZE` is\n",
    "the size of the batches used in stochastic optimization algorithms\n",
    "and `NUM_EPOCHS` the number of times we are going thought the entire\n",
    "training set during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d571ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <answer>\n",
    "EMBEDDING_DIM = 8\n",
    "SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 10\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907d78d",
   "metadata": {},
   "source": [
    "We first need a tokenizer that take a text a returns a list of\n",
    "tokens. There are many tokenizers available from other libraries.\n",
    "Here we use the one that comes with Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a57107",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokenizer(\"All your base are belong to us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd03d1",
   "metadata": {},
   "source": [
    "## Building the vocabulary\n",
    "\n",
    "Then we need to define the set of words that will be understood by\n",
    "the model: this is the vocabulary. We build it from the training\n",
    "set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter: Iterable) -> List[str]:\n",
    "    for data_sample in data_iter:\n",
    "        yield tokenizer(data_sample[1])\n",
    "\n",
    "\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(TRAIN_SET),\n",
    "    min_freq=10,\n",
    "    specials=special_tokens,\n",
    "    special_first=True)\n",
    "UNK_IDX, PAD_IDX = vocab.lookup_indices(special_tokens)\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "vocab['plenty']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d3890",
   "metadata": {},
   "source": [
    "\n",
    "To limit the number of tokens in the vocabulary, we specified\n",
    "`min_freq=10`: a token should be seen at least 10 times to be part\n",
    "of the vocabulary. Consequently some words in the training set (and\n",
    "in the test set) are not present in the vocabulary. We then need to\n",
    "set a default index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a2a74",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# vocab['pouet']                  # Error\n",
    "vocab.set_default_index(UNK_IDX)\n",
    "vocab['pouet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44e13f",
   "metadata": {},
   "source": [
    "# Collate function\n",
    "\n",
    "The collate function maps raw samples coming from the dataset to\n",
    "padded tensors of numericalized tokens ready to be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db28dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List):\n",
    "    def collate(text):\n",
    "        \"\"\"Turn a text into a tensor of integers.\"\"\"\n",
    "\n",
    "        tokens = tokenizer(text)[:SEQ_LENGTH]\n",
    "        return torch.LongTensor(vocab(tokens))\n",
    "\n",
    "    src_batch = [collate(text) for _, text in batch]\n",
    "\n",
    "    # Pad list of tensors using `pad_sequence`\n",
    "    # <answer>\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    # </answer>\n",
    "\n",
    "    # Turn 2 (positive review) and 1 (negative review) labels into 1 and 0\n",
    "    # <answer>\n",
    "    tgt_batch = torch.Tensor([label - 1 for label, _ in batch])\n",
    "    # </answer>\n",
    "\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "\n",
    "print(f\"Number of training examples: {len(TRAIN_SET)}\")\n",
    "print(f\"Number of testing examples: {len(TEST_SET)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e88720",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn([\n",
    "    (1, \"i am Groot\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a6fcf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Training a linear classifier with an embedding\n",
    "\n",
    "We first test a simple linear classifier on the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7c91c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, seq_length):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Define an embedding of `vocab_size` words into a vector space\n",
    "        # of dimension `embedding_dim`.\n",
    "        # <answer>\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        # </answer>\n",
    "\n",
    "        # Define a linear layer from dimension `seq_length` *\n",
    "        # `embedding_dim` to 1.\n",
    "        # <answer>\n",
    "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
    "        # </answer>\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is of size `seq_length` * `batch_size`\n",
    "\n",
    "        # Compute the embedding `embedded` of the batch `x`. `embedded` is\n",
    "        # of size `batch_size` * `seq_length` * `embedding_dim`\n",
    "        # <answer>\n",
    "        embedded = self.embedding(x)\n",
    "        # </answer>\n",
    "\n",
    "        # Flatten the embedded words and feed it to the linear layer.\n",
    "        # `flatten` is of size `batch_size` * (`seq_length` * `embedding_dim`)\n",
    "        # <answer>\n",
    "        flatten = embedded.view(-1, self.seq_length * self.embedding_dim)\n",
    "        # </answer>\n",
    "\n",
    "        # Apply the linear layer and return a squeezed version\n",
    "        # `l1` is of size `batch_size`\n",
    "        # <answer>\n",
    "        return self.l1(flatten).squeeze()\n",
    "        # </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e8282",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We need to implement an accuracy function to be used in the `Trainer`\n",
    "class (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    # `predictions` and `labels` are both tensors of same length\n",
    "\n",
    "    # Implement accuracy\n",
    "    # <answer>\n",
    "    return torch.sum((torch.sigmoid(predictions) > 0.5).float() == (labels > .5)).item() / len(\n",
    "        predictions\n",
    "    )\n",
    "    # </answer>\n",
    "\n",
    "\n",
    "assert accuracy(torch.Tensor([1, -2, 3]), torch.Tensor([1, 0, 1])) == 1\n",
    "assert accuracy(torch.Tensor([1, -2, -3]), torch.Tensor([1, 0, 1])) == 2 / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb02a80",
   "metadata": {},
   "source": [
    "Train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43387df",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, optimizer: Optimizer):\n",
    "    model.to(device)\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        TRAIN_SET, batch_size=BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    matches = 0\n",
    "    losses = 0\n",
    "    for sequences, labels in train_dataloader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        # Implement a step of the algorithm:\n",
    "        #\n",
    "        # - set gradients to zero\n",
    "        # - forward propagate examples in `batch`\n",
    "        # - compute `loss` with chosen criterion\n",
    "        # - back-propagate gradients\n",
    "        # - gradient step\n",
    "        # <answer>\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(sequences)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        # </answer>\n",
    "\n",
    "        acc = accuracy(predictions, labels)\n",
    "\n",
    "        matches += len(predictions) * acc\n",
    "\n",
    "    return losses / len(TRAIN_SET), matches / len(TRAIN_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b4b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        TEST_SET, batch_size=BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    losses = 0\n",
    "    matches = 0\n",
    "    for sequences, labels in val_dataloader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        predictions = model(sequences)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        acc = accuracy(predictions, labels)\n",
    "        matches += len(predictions) * acc\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(TEST_SET), matches / len(TEST_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496136a3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer):\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = timer()\n",
    "        train_loss, train_acc = train_epoch(model, optimizer)\n",
    "        end_time = timer()\n",
    "        val_loss, val_acc = evaluate(model)\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, \"\n",
    "            f\"Train loss: {train_loss:.3f}, \"\n",
    "            f\"Train acc: {train_acc:.3f}, \"\n",
    "            f\"Val loss: {val_loss:.3f}, \"\n",
    "            f\"Val acc: {val_acc:.3f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    \"Predict sentiment of given sentence according to model\"\n",
    "\n",
    "    tensor, _ = collate_fn([(\"dummy\", sentence)])\n",
    "    prediction = model(tensor)\n",
    "    pred = torch.sigmoid(prediction)\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e07e5f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "embedding_net = EmbeddingNet(VOCAB_SIZE, EMBEDDING_DIM, SEQ_LENGTH)\n",
    "print(sum(torch.numel(e) for e in embedding_net.parameters()))\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "optimizer = Adam(embedding_net.parameters())\n",
    "train(embedding_net, optimizer)\n",
    "\n",
    "\n",
    "# # %% [markdown]\n",
    "# # ## Training a linear classifier with a pretrained embedding\n",
    "# #\n",
    "# # Load a GloVe pretrained embedding instead\n",
    "\n",
    "# Download GloVe word embedding\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=\"100\", cache=torch_cache)\n",
    "\n",
    "# Get token embedding of our `vocab`\n",
    "vocab_vectors = glove.get_vecs_by_tokens(vocab.get_itos())\n",
    "\n",
    "# tot_transferred = 0\n",
    "# for v in vocab_vectors:\n",
    "#     if not v.equal(torch.zeros(100)):\n",
    "#         tot_transferred += 1\n",
    "\n",
    "# tot_transferred, len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVeEmbeddingNet(nn.Module):\n",
    "    def __init__(self, seq_length, vocab_vectors, freeze=True):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Define `embedding_dim` from vocabulary and the pretrained `embedding`.\n",
    "        # <answer>\n",
    "        self.embedding_dim = vocab_vectors.size(1)\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "        # </answer>\n",
    "\n",
    "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is of size batch_size * seq_length\n",
    "\n",
    "        # `embedded` is of size batch_size * seq_length * embedding_dim\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # `flatten` is of size batch_size * (seq_length * embedding_dim)\n",
    "        flatten = embedded.view(-1, self.seq_length * self.embedding_dim)\n",
    "\n",
    "        # L1 is of size batch_size\n",
    "        return self.l1(flatten).squeeze()\n",
    "\n",
    "\n",
    "glove_embedding_net1 = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=True)\n",
    "print(sum(torch.numel(e) for e in glove_embedding_net1.parameters()))\n",
    "\n",
    "optimizer = Adam(glove_embedding_net1.parameters())\n",
    "train(glove_embedding_net1, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99b8f7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Use pretrained embedding without fine-tuning\n",
    "\n",
    "Define model and freeze the embedding\n",
    "<answer>\n",
    "glove_embedding_net1 = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=True)\n",
    "</answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6264780",
   "metadata": {},
   "source": [
    "## Fine-tuning the pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and don't freeze embedding weights\n",
    "# <answer>\n",
    "glove_embedding_net2 = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=False)\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4012315",
   "metadata": {},
   "source": [
    "## Recurrent neural network with frozen pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a138ba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_vectors, freeze=True):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Define pretrained embedding\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "\n",
    "        # Size of input `x_t` from `embedding`\n",
    "        self.embedding_size = self.embedding.embedding_dim\n",
    "        self.input_size = self.embedding_size\n",
    "\n",
    "        # Size of hidden state `h_t`\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define a GRU\n",
    "        # <answer>\n",
    "        self.gru = nn.GRU(input_size=self.input_size, hidden_size=self.hidden_size)\n",
    "        # </answer>\n",
    "\n",
    "        # Linear layer on last hidden state\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # `x` is of size `seq_length` * `batch_size` and `h0` is of size 1\n",
    "        # * `batch_size` * `hidden_size`\n",
    "\n",
    "        # Define first hidden state in not provided\n",
    "        if h0 is None:\n",
    "            # Get batch and define `h0` which is of size 1 *\n",
    "            # `batch_size` * `hidden_size`\n",
    "            # <answer>\n",
    "            batch_size = x.size(1)\n",
    "            h0 = torch.zeros(self.gru.num_layers, batch_size, self.hidden_size).to(device)\n",
    "            # </answer>\n",
    "\n",
    "        # `embedded` is of size `seq_length` * `batch_size` *\n",
    "        # `embedding_dim`\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Define `output` and `hidden` returned by GRU:\n",
    "        #\n",
    "        # - `output` is of size `seq_length` * `batch_size` * `embedding_dim`\n",
    "        #   and gathers all the hidden states along the sequence.\n",
    "        # - `hidden` is of size 1 * `batch_size` * `embedding_dim` and is the\n",
    "        #   last hidden state.\n",
    "        # <answer>\n",
    "        output, hidden = self.gru(embedded, h0)\n",
    "        # </answer>\n",
    "\n",
    "        # Apply a linear layer on the last hidden state to have a\n",
    "        # score tensor of size 1 * `batch_size` * 1, and return a\n",
    "        # tensor of size `batch_size`.\n",
    "        # <answer>\n",
    "        return self.linear(hidden).squeeze()\n",
    "        # </answer>\n",
    "\n",
    "\n",
    "rnn = RNN(hidden_size=100, vocab_vectors=vocab_vectors)\n",
    "print(sum(torch.numel(e) for e in rnn.parameters() if e.requires_grad))\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, rnn.parameters()), lr=0.001)\n",
    "train(rnn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f30551",
   "metadata": {},
   "source": [
    "## CNN based text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796842e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_vectors, freeze=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "        self.embedding_dim = self.embedding.embedding_dim\n",
    "\n",
    "        self.conv_0 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(3, self.embedding_dim)\n",
    "        )\n",
    "        self.conv_1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(4, self.embedding_dim)\n",
    "        )\n",
    "        self.conv_2 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(5, self.embedding_dim)\n",
    "        )\n",
    "        self.linear = nn.Linear(3 * 100, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input `x` is of size `seq_length` * `batch_size`\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # The tensor `embedded` is of size `seq_length` * `batch_size` *\n",
    "        # `embedding_dim` and should be of size `batch_size` *\n",
    "        # (`n_channels`=1) * `seq_length` * `embedding_dim` for the\n",
    "        # convolutional layers. You can use `transpose` and `unsqueeze` to make\n",
    "        # the transformation.\n",
    "        # <answer>\n",
    "        embedded = embedded.transpose(0, 1).unsqueeze(1)\n",
    "        # </answer>\n",
    "\n",
    "        # Tensor `embedded` is now of size `batch_size` * 1 *\n",
    "        # `seq_length` * `embedding_dim` before convolution and should\n",
    "        # be of size `batch_size` * (`out_channels` = 100) *\n",
    "        # (`seq_length` - `kernel_size[0]` + 1) after convolution and\n",
    "        # squeezing.\n",
    "        # Implement the convolution layer\n",
    "        # <answer>\n",
    "        conved_0 = self.conv_0(embedded).squeeze(3)\n",
    "        conved_1 = self.conv_1(embedded).squeeze(3)\n",
    "        conved_2 = self.conv_2(embedded).squeeze(3)\n",
    "        # </answer>\n",
    "\n",
    "        # Non-linearity step, we use ReLU activation\n",
    "        # <answer>\n",
    "        conved_0_relu = F.relu(conved_0)\n",
    "        conved_1_relu = F.relu(conved_1)\n",
    "        conved_2_relu = F.relu(conved_2)\n",
    "        # </answer>\n",
    "\n",
    "        # Max-pooling layer: pooling along whole sequence\n",
    "        # Implement max pooling\n",
    "        # <answer>\n",
    "        seq_len_0 = conved_0_relu.shape[2]\n",
    "        pooled_0 = F.max_pool1d(conved_0_relu, kernel_size=seq_len_0).squeeze(2)\n",
    "\n",
    "        seq_len_1 = conved_1_relu.shape[2]\n",
    "        pooled_1 = F.max_pool1d(conved_1_relu, kernel_size=seq_len_1).squeeze(2)\n",
    "\n",
    "        seq_len_2 = conved_2_relu.shape[2]\n",
    "        pooled_2 = F.max_pool1d(conved_2_relu, kernel_size=seq_len_2).squeeze(2)\n",
    "        # </answer>\n",
    "\n",
    "        # Dropout on concatenated pooled features\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        # Linear layer\n",
    "        return self.linear(cat).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf14e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(vocab_vectors)\n",
    "optimizer = optim.Adam(cnn.parameters())\n",
    "train(cnn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfd609",
   "metadata": {},
   "source": [
    "## Test function"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
