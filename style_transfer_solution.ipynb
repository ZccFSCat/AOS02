{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1fbc5a",
   "metadata": {},
   "source": [
    "# Image Style Transfer Using Convolutional Neural Networks\n",
    "\n",
    "This notebook implements the algorithm found in [(Gatys\n",
    "2016)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a2f6bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as utils\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "imsize = 128\n",
    "loader = transforms.Compose([transforms.Resize(imsize), transforms.ToTensor()])\n",
    "\n",
    "# Load `content_img` as a torch tensor of size 3 * `imsize` * `imsize`\n",
    "image = Image.open(\"./data/images/dancing.jpg\")\n",
    "content_img = loader(image)\n",
    "\n",
    "# Load `style_img` as a torch tensor of size 3 * `imsize` * `imsize`\n",
    "image = Image.open(\"./data/images/mondrian.jpg\")\n",
    "style_img = loader(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f99e5",
   "metadata": {},
   "source": [
    "\n",
    "## Feature extraction with VGG19\n",
    "The next cell is a CNN based on VGG19 which extracts convolutional\n",
    "features specified by `modules_indexes`. It is used to compute the\n",
    "features of the content and style image. It is also used to\n",
    "reconstruct the target image by backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eaf6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19Features(nn.Module):\n",
    "    def __init__(self, modules_indexes):\n",
    "        super(VGG19Features, self).__init__()\n",
    "\n",
    "        # VGG19 pretrained model in evaluation mode\n",
    "        self.vgg19 = models.vgg19(weights=models.VGG19_Weights.DEFAULT).eval()\n",
    "\n",
    "        # Indexes of layers to remember\n",
    "        self.modules_indexes = modules_indexes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Define a hardcoded `mean` and `std` of size 3 * 1 * 1\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "\n",
    "        # First center and normalize `input` with `mean` and `std`\n",
    "        # <answer>\n",
    "        input_norm = (input - mean) / std\n",
    "        # </answer>\n",
    "\n",
    "        # Add a fake mini-batch dimension to `input_norm`\n",
    "        # <answer>\n",
    "        input_norm = input_norm.unsqueeze(0)\n",
    "        # </answer>\n",
    "\n",
    "        # Install hooks on specified modules to save their features\n",
    "        features = []\n",
    "        handles = []\n",
    "        for module_index in self.modules_indexes:\n",
    "\n",
    "            def hook(module, input, output):\n",
    "                # `output` is of size (`batchsize` = 1) * `n_filters`\n",
    "                # * `imsize` * `imsize`\n",
    "                features.append(output)\n",
    "\n",
    "            handle = self.vgg19.features[module_index].register_forward_hook(hook)\n",
    "            handles.append(handle)\n",
    "\n",
    "        # Forward propagate `input_norm`. This will trigger the hooks\n",
    "        # set up above and populate `features`\n",
    "        self.vgg19(input_norm)\n",
    "\n",
    "        # Remove hooks\n",
    "        [handle.remove() for handle in handles]\n",
    "\n",
    "        # The output of our custom VGG19Features neural network is a\n",
    "        # list of features of `input`\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8498af6",
   "metadata": {},
   "source": [
    "\n",
    "The next cell defines the convolutional layers we will use to\n",
    "capture the style and the content. Look at the paper to see what are\n",
    "those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e979b3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Indexes of interesting features to extract\n",
    "\n",
    "# Define `modules_indexes`\n",
    "# <answer>\n",
    "modules_indexes = [0, 2, 5, 7, 10]\n",
    "# </answer>\n",
    "\n",
    "vgg19 = VGG19Features(modules_indexes)\n",
    "content_features = [f.detach() for f in vgg19.forward(content_img)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b8f62",
   "metadata": {},
   "source": [
    "\n",
    "## Style features as gram matrix of convolutional features\n",
    "\n",
    "The next cell computes the gram matrix of `input`. We first need to\n",
    "reshape `input` before computing the gram matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca4eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input):\n",
    "    batchsize, n_filters, width, height = input.size()\n",
    "\n",
    "    # Reshape `input` into `n_filters` * `n_pixels`\n",
    "    # <answer>\n",
    "    features = input.view(n_filters, width*height)\n",
    "    # </answer>\n",
    "\n",
    "    # Compute the inner products between filters in `G`\n",
    "    # <answer>\n",
    "    G = torch.mm(features, features.t())\n",
    "    # </answer>\n",
    "\n",
    "    # We `normalize` the values of the gram matrix by dividing by the\n",
    "    # number of element in each feature maps.\n",
    "    return G.div(n_filters * width * height)\n",
    "\n",
    "\n",
    "style_gram_features = [gram_matrix(f.detach()) for f in vgg19.forward(style_img)]\n",
    "\n",
    "target = content_img.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee36843",
   "metadata": {},
   "source": [
    "\n",
    "## Optimizer\n",
    "\n",
    "Look at the paper to see what is the algorithm they are using.\n",
    "Remember that we are optimizing on a target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f55765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define `optimizer` to use L-BFGS algorithm to do gradient descent on\n",
    "# `target`\n",
    "# <answer>\n",
    "optimizer = optim.LBFGS([target])\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aac5e4",
   "metadata": {},
   "source": [
    "\n",
    "## The algorithm\n",
    "\n",
    "From the paper, there are two different losses. The style loss and the\n",
    "content loss.\n",
    "\n",
    "Define `style_weight` the trade-off parameter between style and\n",
    "content losses\n",
    "<answer>\n",
    "style_weight = 10**6\n",
    "</answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b4f4161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0:\n",
      "Style Loss: 10.415602 Content Loss: 99735.046875 Overall: 10515337.000000\n",
      "step 10:\n",
      "Style Loss: 0.736319 Content Loss: 337936.906250 Overall: 1074256.125000\n",
      "step 20:\n",
      "Style Loss: 0.436361 Content Loss: 388684.531250 Overall: 825046.000000\n",
      "step 30:\n",
      "Style Loss: 0.363572 Content Loss: 400106.750000 Overall: 763678.562500\n",
      "step 40:\n",
      "Style Loss: 0.334233 Content Loss: 403407.687500 Overall: 737640.312500\n",
      "step 50:\n",
      "Style Loss: 0.318921 Content Loss: 404121.593750 Overall: 723042.312500\n",
      "step 60:\n",
      "Style Loss: 0.308954 Content Loss: 403457.312500 Overall: 712411.625000\n",
      "step 70:\n",
      "Style Loss: 0.300849 Content Loss: 403455.500000 Overall: 704304.250000\n",
      "step 80:\n",
      "Style Loss: 0.294807 Content Loss: 403440.718750 Overall: 698248.125000\n",
      "step 90:\n",
      "Style Loss: 0.289846 Content Loss: 403200.343750 Overall: 693046.375000\n",
      "step 100:\n",
      "Style Loss: 0.285253 Content Loss: 403203.031250 Overall: 688456.375000\n",
      "step 110:\n",
      "Style Loss: 0.281025 Content Loss: 403289.125000 Overall: 684314.062500\n",
      "step 120:\n",
      "Style Loss: 0.277805 Content Loss: 403183.250000 Overall: 680988.125000\n",
      "step 130:\n",
      "Style Loss: 0.275576 Content Loss: 402973.187500 Overall: 678549.500000\n",
      "step 140:\n",
      "Style Loss: 0.273528 Content Loss: 402795.031250 Overall: 676322.750000\n",
      "step 150:\n",
      "Style Loss: 0.271898 Content Loss: 402535.406250 Overall: 674433.375000\n",
      "step 160:\n",
      "Style Loss: 0.270446 Content Loss: 402383.937500 Overall: 672829.875000\n",
      "step 170:\n",
      "Style Loss: 0.268976 Content Loss: 402333.468750 Overall: 671309.000000\n",
      "step 180:\n",
      "Style Loss: 0.267712 Content Loss: 402320.437500 Overall: 670032.375000\n",
      "step 190:\n",
      "Style Loss: 0.266550 Content Loss: 402334.687500 Overall: 668884.625000\n",
      "step 200:\n",
      "Style Loss: 0.265750 Content Loss: 402221.468750 Overall: 667971.000000\n",
      "step 210:\n",
      "Style Loss: 0.264996 Content Loss: 402075.906250 Overall: 667071.875000\n",
      "step 220:\n",
      "Style Loss: 0.264091 Content Loss: 402078.375000 Overall: 666169.250000\n",
      "step 230:\n",
      "Style Loss: 0.263259 Content Loss: 402113.250000 Overall: 665371.875000\n",
      "step 240:\n",
      "Style Loss: 0.262605 Content Loss: 402119.375000 Overall: 664724.125000\n",
      "step 250:\n",
      "Style Loss: 0.262054 Content Loss: 402102.906250 Overall: 664157.062500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# </answer>\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Gradient step : don't forget to pass the closure to the optimizer\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# <answer>\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# </answer>\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/d2l/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/d2l/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/d2l/lib/python3.11/site-packages/torch/optim/lbfgs.py:383\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_old \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    382\u001b[0m     al[i] \u001b[38;5;241m=\u001b[39m old_stps[i]\u001b[38;5;241m.\u001b[39mdot(q) \u001b[38;5;241m*\u001b[39m ro[i]\n\u001b[0;32m--> 383\u001b[0m     q\u001b[38;5;241m.\u001b[39madd_(old_dirs[i], alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mal[i])\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# multiply by initial Hessian\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# r/d is the final direction\u001b[39;00m\n\u001b[1;32m    387\u001b[0m d \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(q, H_diag)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for step in range(500):\n",
    "    # To keep track of the losses in the closure\n",
    "    losses = {}\n",
    "\n",
    "    # Need to use a closure that computes the loss and gradients to allow the\n",
    "    # optimizer to evaluate repeatedly at different locations\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # First, forward propagate `target` through our VGG19Features neural\n",
    "        # network and store its output as `target_features`\n",
    "        # <answer>\n",
    "        target_features = vgg19(target)\n",
    "        # </answer>\n",
    "\n",
    "        # Define `content_loss` on the first layer only\n",
    "        # <answer>\n",
    "        content_loss = torch.sum((target_features[0] - content_features[0])**2)\n",
    "        # </answer>\n",
    "\n",
    "        style_loss = 0\n",
    "        for target_feature, style_gram_feature in zip(target_features, style_gram_features):\n",
    "            # Compute Gram matrix\n",
    "            # <answer>\n",
    "            target_gram_feature = gram_matrix(target_feature)\n",
    "            # </answer>\n",
    "\n",
    "            # Add current loss to `style_loss`\n",
    "            # <answer>\n",
    "            style_loss += torch.sum((target_gram_feature - style_gram_feature)**2)\n",
    "            # </answer>\n",
    "\n",
    "        # Compute combined loss\n",
    "        # <answer>\n",
    "        style_weight = 10**6\n",
    "        loss = content_loss + style_weight * style_loss\n",
    "        # </answer>\n",
    "\n",
    "        # Store the losses\n",
    "        losses[\"loss\"] = loss.item()\n",
    "        losses[\"style_loss\"] = style_loss.item()\n",
    "        losses[\"content_loss\"] = content_loss.item()\n",
    "\n",
    "        # Backward propagation and return loss\n",
    "        # <answer>\n",
    "        loss.backward()\n",
    "        return loss\n",
    "        # </answer>\n",
    "\n",
    "    # Gradient step : don't forget to pass the closure to the optimizer\n",
    "    # <answer>\n",
    "    optimizer.step(closure)\n",
    "    # </answer>\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(\"step {}:\".format(step))\n",
    "        print(\n",
    "            \"Style Loss: {:4f} Content Loss: {:4f} Overall: {:4f}\".format(\n",
    "                losses[\"style_loss\"], losses[\"content_loss\"], losses[\"loss\"]\n",
    "            )\n",
    "        )\n",
    "        img = target.clone().squeeze()\n",
    "        img = img.clamp_(0, 1)\n",
    "        utils.save_image(img, \"output-{}.png\".format(step))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
