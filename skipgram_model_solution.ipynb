{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d5aeed",
   "metadata": {},
   "source": [
    "# Skipgram model trained on \"20000 lieues sous les mers\"\n",
    "\n",
    "## Needed libraries\n",
    "\n",
    "You will need the following new libraries:\n",
    "- `spacy` for tokenizing\n",
    "- `gensim` for cosine similarities (use `gensim>=4.0.0`)\n",
    "\n",
    "You will also need to download rules for tokenizing a french text.\n",
    "```python\n",
    "python -m spacy download fr_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95af5fd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "spacy_fr = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730053e6",
   "metadata": {},
   "source": [
    "## Tokenizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e025f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a french tokenizer to Create a tokenizer for the french language\n",
    "with open(\"data/20_000_lieues_sous_les_mers.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    document = spacy_fr.tokenizer(f.read())\n",
    "\n",
    "# Define a filtered set of tokens by iterating on `document`. Define a\n",
    "# subset of tokens that are\n",
    "#\n",
    "# - alphanumeric\n",
    "# - in lower case\n",
    "# <answer>\n",
    "tokens = [\n",
    "    tok.text.lower()\n",
    "    for tok in document if tok.is_alpha or tok.is_digit\n",
    "]\n",
    "# </answer>\n",
    "\n",
    "# Make a list of unique tokens and dictionary that maps tokens to\n",
    "# their index in that list.\n",
    "# <answer>\n",
    "idx2tok = list(set(tokens))\n",
    "tok2idx = {token: i for i, token in enumerate(idx2tok)}\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ab7f0",
   "metadata": {},
   "source": [
    "## The continuous bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34fa0d8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Define an Embedding module (`nn.Embedding`) and a linear\n",
    "        # transform (`nn.Linear`) without bias.\n",
    "        # <answer>\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.U_transpose = nn.Linear(self.embedding_size, self.vocab_size, bias=False)\n",
    "        # </answer>\n",
    "\n",
    "    def forward(self, center):\n",
    "        # Implements the forward pass of the skipgram model\n",
    "        # `center` is of size `batch_size`\n",
    "\n",
    "        # `e_i` is of size `batch_size` * `embedding_size`\n",
    "        # <answer>\n",
    "        e_i = self.embeddings(center)\n",
    "        # </answer>\n",
    "\n",
    "        # `UT_e_i` is of size `batch_size` * `vocab_size`\n",
    "        # <answer>\n",
    "        UT_e_i = self.U_transpose(e_i)\n",
    "        # </answer>\n",
    "\n",
    "        # <answer>\n",
    "        return UT_e_i\n",
    "        # </answer>\n",
    "\n",
    "\n",
    "# Set the size of vocabulary and size of embedding\n",
    "VOCAB_SIZE = len(idx2tok)\n",
    "EMBEDDING_SIZE = 32\n",
    "\n",
    "# Create a Continuous bag of words model\n",
    "skipgram = Skipgram(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "\n",
    "# Send to GPU if any\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "skipgram.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd2782",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e39977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n-grams for a given list of tokens, use yield, use window length of n-grams\n",
    "def ngrams_iterator(token_list, ngrams):\n",
    "    \"\"\"Generates successive N-grams from a list of tokens.\"\"\"\n",
    "\n",
    "    for i in range(len(token_list) - ngrams + 1):\n",
    "        idxs = [tok2idx[tok] for tok in token_list[i:i+ngrams]]\n",
    "\n",
    "        # Get center element in `idxs`\n",
    "        center = idxs.pop(ngrams // 2)\n",
    "\n",
    "        # Yield the index of center word and indexes of context words\n",
    "        # as a Numpy array (for Pytorch to automatically convert it to\n",
    "        # a Tensor).\n",
    "        yield center, np.array(idxs)\n",
    "\n",
    "\n",
    "# Create center, context data\n",
    "NGRAMS = 5\n",
    "ngrams = list(ngrams_iterator(tokens, NGRAMS))\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "data = torch.utils.data.DataLoader(ngrams, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee0b37",
   "metadata": {},
   "source": [
    "## Learn Skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Adam algorithm on the parameters of `skipgram` with a learning\n",
    "# rate of 0.01\n",
    "# <answer>\n",
    "optimizer = optim.Adam(skipgram.parameters(), lr=0.01)\n",
    "# </answer>\n",
    "\n",
    "# Use a cross-entropy loss from the `nn` submodule\n",
    "# <answer>\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cfd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    for i, (center, context) in enumerate(data):\n",
    "        center, context = center.to(device), context.to(device)\n",
    "\n",
    "        # Reset the gradients of the computational graph\n",
    "        # <answer>\n",
    "        skipgram.zero_grad()\n",
    "        # </answer>\n",
    "\n",
    "        # Forward pass\n",
    "        # <answer>\n",
    "        UT_e_i = skipgram.forward(center)\n",
    "        # </answer>\n",
    "\n",
    "        # Define one-hot encoding for tokens in context. `one_hots` has the same\n",
    "        # size as `UT_e_i` and is zero everywhere except at location\n",
    "        # corresponding to `context`. You can use `torch.scatter`.\n",
    "        # <answer>\n",
    "        one_hots = torch.zeros_like(UT_e_i).scatter(1, context, 1/(NGRAMS-1))\n",
    "        # </answer>\n",
    "\n",
    "        # Compute loss between `UT_e_i` and `one_hots`\n",
    "        # <answer>\n",
    "        loss = ce_loss(UT_e_i, one_hots)\n",
    "        # </answer>\n",
    "\n",
    "        # Backward pass to compute gradients of each parameter\n",
    "        # <answer>\n",
    "        loss.backward()\n",
    "        # </answer>\n",
    "\n",
    "        # Gradient descent step according to the chosen optimizer\n",
    "        # <answer>\n",
    "        optimizer.step()\n",
    "        # </answer>\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            loss_avg = float(total_loss / (i + 1))\n",
    "            print(\n",
    "                f\"Epoch ({epoch}/{EPOCHS}), batch: ({i}/{len(data)}), loss: {loss_avg}\"\n",
    "            )\n",
    "\n",
    "    # Print average loss after each epoch\n",
    "    loss_avg = float(total_loss / len(data))\n",
    "    print(\"{}/{} loss {:.2f}\".format(epoch, EPOCHS, loss_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f99c6",
   "metadata": {},
   "source": [
    "## Prediction functions\n",
    "\n",
    "Now that the skipgram model is learned we can give it a word and see what\n",
    "context the model predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5905fd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def predict_context_words(skipgram, center_word, k=4):\n",
    "    \"\"\"Predicts `k` best context words of `center_word` according to model `skipgram`\"\"\"\n",
    "\n",
    "    # Get index of `center_word`\n",
    "    center_word_idx = tok2idx[center_word]\n",
    "\n",
    "    # Create a fake minibatch containing just `center_word_idx`. Make sure that\n",
    "    # `fake_minibatch` is a Long tensor and don't forget to send it to device.\n",
    "    # <answer>\n",
    "    fake_minibatch = torch.LongTensor([center_word_idx]).unsqueeze(0).to(device)\n",
    "    # </answer>\n",
    "\n",
    "    # Forward propagate through the skipgram model\n",
    "    # <answer>\n",
    "    score_context = skipgram(fake_minibatch).squeeze()\n",
    "    # </answer>\n",
    "\n",
    "    # Retrieve top k-best indexes using `torch.topk`\n",
    "    # <answer>\n",
    "    _, best_idxs = torch.topk(score_context, k=k)\n",
    "    # </answer>\n",
    "\n",
    "    # Return actual tokens using `idx2tok`\n",
    "    # <answer>\n",
    "    return [idx2tok[idx] for idx in best_idxs]\n",
    "    # </answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c3dfd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "predict_context_words(skipgram, \"mille\")\n",
    "predict_context_words(skipgram, \"nemo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad6c91",
   "metadata": {},
   "source": [
    "## Testing the embedding\n",
    "\n",
    "We use the library `gensim` to easily compute most similar words for\n",
    "the embedding we just learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97621040",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = KeyedVectors(vector_size=EMBEDDING_SIZE)\n",
    "m.add_vectors(idx2tok, skipgram.embeddings.weight.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2290bc",
   "metadata": {},
   "source": [
    "You can now test most similar words for, for example \"lieues\",\n",
    "\"mers\", \"professeur\"... You can look at `words_decreasing_freq` to\n",
    "test most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf85625",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, freq = np.unique(tokens, return_counts=True)\n",
    "idxs = freq.argsort()[::-1]\n",
    "words_decreasing_freq = list(zip(unique[idxs], freq[idxs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2742972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <answer>\n",
    "m.most_similar(\"lieues\")\n",
    "m.most_similar(\"professeur\")\n",
    "m.most_similar(\"mers\")\n",
    "m.most_similar(\"noire\")\n",
    "m.most_similar(\"mètres\")\n",
    "m.most_similar(\"ma\")\n",
    "# </answer>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
